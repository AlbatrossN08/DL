Aim:- To design and implement Long Short-Term Memory (LSTM) 
model for Sequence prediction.

Theory:
Aim:- To design and implement Long Short-Term Memory (LSTM) model for Sequence prediction.

Thoery:
RNN:
A Recurrent Neural Network (RNN) is a type of artificial neural network designed 
for processing and modeling sequential data. Unlike feedforward neural networks, 
which process data in a single pass, RNNs have loops and can maintain a hidden 
state that captures information about previous time steps. This recurrent nature
makes them well-suited for tasks involving time series, natural language processing,
speech recognition, and more.
Recurrent Neural Networks (RNNs) are commonly used for sequence prediction tasks, 
where the goal is to generate an output sequence based on an input sequence.
Sequence prediction tasks can be found in various domains, including natural 
language processing, time series forecasting, speech recognition, and more.
Long Short-Term Memory (LSTM) is a type of recurrent neural network (RNN) 
architecture that is specifically designed to address the vanishing gradient 
problem, which is a limitation of traditional RNNs. 
Diagram of RNN Architecture: 
https://www.researchgate.net/figure/The-architecture-of-RNN_fig3_312593525
	
Steps:
Define numerical sequence ‘data’
2. Use a function to split the sequence into input and output
3. Prepare the input data for LSTM layer
4. Create a sequential model with an LSTM layer and dense output
5. Define the models optimizers loss function and metrics
6. Train model on the data
7. Use the trained model to predict the next number in a test sequence


Steps:
Define numerical sequence ‘data’
2. Use a function to split the sequence into input and output
3. Prepare the input data for LSTM layer
4. Create a sequential model with an LSTM layer and dense output
5. Define the models optimizers loss function and metrics
6. Train model on the data
7. Use the trained model to predict the next number in a test sequence

